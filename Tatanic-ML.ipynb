{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063e3ed3-9e72-450a-949d-cac740228159",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6206dd-3b7a-4faf-aac5-3f0a5f401f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries for Data Preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Import necessary libraries for Data Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import necessary libraries for Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a01586-e21a-474d-a9dd-5fb101df9e4b",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebfc49-7aaa-4abf-abb5-744e29d8d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_data = pd.read_csv('test.csv', sep=',')\n",
    "\n",
    "# Load the train data\n",
    "train_data = pd.read_csv('train.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076370f-ceb7-4053-854a-ccdb87f78892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the train data\n",
    "print(train_data.info())\n",
    "print(train_data.iloc[1])\n",
    "\n",
    "# Display information about the test data\n",
    "print(test_data.info())\n",
    "print(test_data.iloc[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10bf5ce-04c3-4ac8-9792-47068780ccce",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff6b36-614f-4dc0-9a63-bd69afd3aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove not necessary column from the training and testing datasets\n",
    "train_data = train_data.drop('Name', axis=1)\n",
    "test_data = test_data.drop('Name', axis=1)\n",
    "train_data = train_data.drop('Ticket', axis=1)\n",
    "test_data = test_data.drop('Ticket', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd329d2e-29fa-4601-adf9-7e03b651c825",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dcb5de-4577-45fd-8f4e-bf7188ff5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in the 'Age' column with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "train_data['Age'] = imputer.fit_transform(train_data[['Age']])\n",
    "test_data['Age'] = imputer.transform(test_data[['Age']])\n",
    "\n",
    "# Drop columns with too many missing values (e.g., 'Cabin')\n",
    "train_data.drop('Cabin', axis=1, inplace=True)\n",
    "test_data.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "# Replace missing values in the 'Embarked' column with the most frequent value\n",
    "most_frequent_embarked = train_data['Embarked'].mode()[0]\n",
    "train_data['Embarked'].fillna(most_frequent_embarked, inplace=True)\n",
    "test_data['Embarked'].fillna(most_frequent_embarked, inplace=True)\n",
    "\n",
    "# Replace missing values in the 'Fare' column with the median\n",
    "most_frequent_Fare = train_data['Fare'].median()\n",
    "train_data['Fare'].fillna(most_frequent_Fare, inplace=True)\n",
    "test_data['Fare'].fillna(most_frequent_Fare, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ddbbb-1779-4490-bd9e-b003d6d1e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in train_data\n",
    "missing_values_train = train_data.isnull().sum()\n",
    "\n",
    "# Count missing values in test_data\n",
    "missing_values_test = test_data.isnull().sum()\n",
    "\n",
    "print(\"Missing values in train_data:\\n\", missing_values_train)\n",
    "print(\"\\nMissing values in test_data:\\n\", missing_values_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7cb76-04ae-4117-91bd-83b35b390121",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0ac43-7632-49d6-8f7a-5fffa0b96972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the 'Sex', 'Embarked', and 'Pclass' columns in the training data\n",
    "train_data_encoded = pd.DataFrame(encoder.fit_transform(train_data[['Sex', 'Embarked', 'Pclass']]).toarray(), columns=encoder.get_feature_names_out(['Sex', 'Embarked', 'Pclass']))\n",
    "\n",
    "# Concatenate the encoded columns with the rest of the data\n",
    "train_data = pd.concat([train_data.drop(['Sex', 'Embarked', 'Pclass'], axis=1), train_data_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be426e-399b-481d-afce-c63d75422784",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5a1a4-0813-40ee-97a4-1a75c1077379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the 'Fare' column in the training data\n",
    "train_data['Fare'] = scaler.fit_transform(train_data[['Fare']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83d0dd-497e-4e21-970b-a8e06917a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdcb00a-9164-4bc7-8bdf-01bcbdb887bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a40d41-854e-45c3-a8b3-763664b7ba44",
   "metadata": {},
   "source": [
    "### Correalation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dab8a8-c1c2-4391-8430-3947e56f0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392dc131-d507-47a2-b98d-edc3fc8339c8",
   "metadata": {},
   "source": [
    "#### Statistically significant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f932172-a0ae-40eb-bdbe-9280f25a3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation coefficient and p-value for each column\n",
    "for column in data.columns:\n",
    "    if column != 'Survived':\n",
    "        correlation, p_value = stats.pearsonr(data[column], data['Survived'])\n",
    "        print(f\"Column: {column}\")\n",
    "        print(f\"Correlation Coefficient: {correlation:.2f}\")\n",
    "        print(f\"P-value: {p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(\"The correlation is statistically significant.\")\n",
    "        else:\n",
    "            print(\"The correlation is not statistically significant.\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da87a8-5e19-473b-a07c-28637fed562e",
   "metadata": {},
   "source": [
    "#### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864f6bb-22ec-44bd-b1d2-5b7a28b09319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop(['PassengerId', 'Survived'], axis=1)\n",
    "y = data['Survived']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a bar chart of the feature importances\n",
    "plt.barh(X_train.columns, importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9514c-8c25-43e3-b8cd-b57b44810df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9872a-e7bf-4614-84e6-e8942a0bf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specified columns from the DataFrame\n",
    "data = data.drop(['Embarked_C', 'Embarked_Q', 'Embarked_S', 'Pclass_1', 'Pclass_2'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfbb98-de54-4f6b-9831-6b05995855ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40fa599-2761-4f86-bd3a-13c69c7d2985",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377088b9-224f-43b6-81b2-0fe4913e4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grids for each model\n",
    "param_grid_logreg = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'sag', 'saga', 'newton-cg']\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the models\n",
    "logreg = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the GridSearchCV objects\n",
    "grid_search_logreg = GridSearchCV(estimator=logreg, param_grid=param_grid_logreg, cv=5, scoring='accuracy')\n",
    "grid_search_dt = GridSearchCV(estimator=dt, param_grid=param_grid_dt, cv=5, scoring='accuracy')\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV objects to the data\n",
    "grid_search_logreg.fit(X_train, y_train)\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding accuracy score for each model\n",
    "print(\"Logistic Regression Best Parameters: \", grid_search_logreg.best_params_)\n",
    "print(\"Logistic Regression Best Accuracy: \", grid_search_logreg.best_score_)\n",
    "print(\"Decision Tree Best Parameters: \", grid_search_dt.best_params_)\n",
    "print(\"Decision Tree Best Accuracy: \", grid_search_dt.best_score_)\n",
    "print(\"Random Forest Best Parameters: \", grid_search_rf.best_params_)\n",
    "print(\"Random Forest Best Accuracy: \", grid_search_rf.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813c930-a8b8-433e-a706-c9552e2be713",
   "metadata": {},
   "source": [
    "The best parameters for each model were chosen using grid search with cross-validation. Grid search is a technique for hyperparameter tuning that involves training a model on a grid of different hyperparameter values and selecting the combination that results in the best performance on a validation set.\n",
    "\n",
    "For Logistic Regression, the best regularization parameter (C) was found to be 0.1 and the best solver was 'newton-cg'. The regularization parameter C controls the trade-off between fitting the training data well and avoiding overfitting. A smaller value of C results in a simpler model that may underfit the data, while a larger value of C results in a more complex model that may overfit the data. The 'newton-cg' solver is a good choice for large datasets and can handle multiclass problems.\n",
    "\n",
    "For Decision Tree, the best criterion was found to be 'entropy', the best maximum depth was 5, and the best minimum samples split was 10. The criterion 'entropy' measures the impurity of a node based on the entropy of the class distribution, while 'gini' measures the impurity based on the Gini impurity. The maximum depth parameter controls the maximum number of decision levels in the tree, while the minimum samples split parameter controls the minimum number of samples required to split an internal node.\n",
    "\n",
    "For Random Forest, the best maximum depth was found to be None (i.e., no maximum depth), the best maximum features was 'sqrt', the best minimum samples split was 5, and the best number of estimators was 200. The maximum depth parameter controls the maximum number of decision levels in each tree, while the maximum features parameter controls the maximum number of features considered when splitting a node. The minimum samples split parameter controls the minimum number of samples required to split an internal node, and the number of estimators parameter controls the number of trees in the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a822a1-ccc3-4e47-a842-2461fa1898d7",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a4fdb-21a6-4952-a8fb-dce1d1c280b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_val)\n",
    "print('Logistic Regression Accuracy:', round(accuracy_score(y_val, y_pred_logreg), 2))\n",
    "print('Logistic Regression Precision:', round(precision_score(y_val, y_pred_logreg), 2))\n",
    "print('Logistic Regression Recall:', round(recall_score(y_val, y_pred_logreg), 2))\n",
    "print('Logistic Regression F1-Score:', round(f1_score(y_val, y_pred_logreg), 2))\n",
    "cm_logreg = confusion_matrix(y_val, y_pred_logreg)\n",
    "sns.heatmap(cm_logreg, annot=True, cmap='Blues')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve and AUC for Logistic Regression\n",
    "y_pred_proba_logreg = logreg.predict_proba(X_val)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba_logreg)\n",
    "roc_auc = round(auc(fpr, tpr), 2)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label = 'Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_val)\n",
    "print('Decision Tree Accuracy:', round(accuracy_score(y_val, y_pred_dt), 2))\n",
    "print('Decision Tree Precision:', round(precision_score(y_val, y_pred_dt), 2))\n",
    "print('Decision Tree Recall:', round(recall_score(y_val, y_pred_dt), 2))\n",
    "print('Decision Tree F1-Score:', round(f1_score(y_val, y_pred_dt), 2))\n",
    "cm_dt = confusion_matrix(y_val, y_pred_dt)\n",
    "sns.heatmap(cm_dt, annot=True, cmap='Blues')\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve and AUC for Decision Tree\n",
    "y_pred_proba_dt = dt.predict_proba(X_val)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba_dt)\n",
    "roc_auc = round(auc(fpr, tpr), 2)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label = 'Decision Tree (area = %0.2f)' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "print('Random Forest Accuracy:', round(accuracy_score(y_val, y_pred_rf), 2))\n",
    "print('Random Forest Precision:', round(precision_score(y_val, y_pred_rf), 2))\n",
    "print('Random Forest Recall:', round(recall_score(y_val, y_pred_rf), 2))\n",
    "print('Random Forest F1-Score:', round(f1_score(y_val, y_pred_rf), 2))\n",
    "cm_rf = confusion_matrix(y_val, y_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, cmap='Blues')\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve and AUC for Random Forest\n",
    "y_pred_proba_rf = rf.predict_proba(X_val)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba_rf)\n",
    "roc_auc = round(auc(fpr, tpr), 2)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label = 'Random Forest (area = %0.2f)' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
